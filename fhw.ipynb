{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fhw.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "NsxNA2CTP-Nk"
      ],
      "authorship_tag": "ABX9TyMHsobtXcmqgHOIL/oTqJq2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aSafarpoor/test/blob/master/fhw.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvO4M6MnJmpR"
      },
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "from keras.utils.np_utils import to_categorical\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "import tensorflow as tf"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhXYs53qTU_a"
      },
      "source": [
        "# read data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1f0dfO-VJxGL"
      },
      "source": [
        "# x, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
        "# x = (x/255).astype('float32')\n",
        "# y = to_categorical(y)\n",
        "\n",
        "# x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.15, random_state=42)\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbvL_5_WPrqT"
      },
      "source": [
        "x_train = x_train.reshape(x_train.shape[0],28*28)/255\n",
        "x_test = x_test.reshape(x_test.shape[0],28*28)/255"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsxNA2CTP-Nk"
      },
      "source": [
        "# class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6QbuvpKKr6j",
        "outputId": "ea461e02-1f2b-4a2e-f283-2970748e7a29"
      },
      "source": [
        "# from sklearn.datasets import fetch_openml\n",
        "# from keras.utils.np_utils import to_categorical\n",
        "# import numpy as np\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# import time\n",
        "\n",
        "# x, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
        "# x = (x/255).astype('float32')\n",
        "# y = to_categorical(y)\n",
        "\n",
        "# x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.15, random_state=42)\n",
        "\n",
        "class DeepNeuralNetwork():\n",
        "    def __init__(self, sizes, epochs=3, l_rate=0.1):\n",
        "        self.sizes = sizes\n",
        "        self.epochs = epochs\n",
        "        self.l_rate = l_rate\n",
        "\n",
        "        # we save all parameters in the neural network in this dictionary\n",
        "        self.params = self.initialization()\n",
        "\n",
        "    def sigmoid(self, x, derivative=False):\n",
        "        if derivative:\n",
        "            return (np.exp(-x))/((np.exp(-x)+1)**2)\n",
        "        return 1/(1 + np.exp(-x))\n",
        "\n",
        "    def softmax(self, x, derivative=False):\n",
        "        # Numerically stable with large exponentials\n",
        "        exps = np.exp(x - x.max())\n",
        "        if derivative:\n",
        "            return exps / np.sum(exps, axis=0) * (1 - exps / np.sum(exps, axis=0))\n",
        "        return exps / np.sum(exps, axis=0)\n",
        "\n",
        "    def initialization(self):\n",
        "        # number of nodes in each layer\n",
        "        input_layer=self.sizes[0]\n",
        "        hidden_1=self.sizes[1]\n",
        "        hidden_2=self.sizes[2]\n",
        "        output_layer=self.sizes[3]\n",
        "\n",
        "        params = {\n",
        "            'W1':np.random.randn(hidden_1, input_layer) * np.sqrt(1. / hidden_1),\n",
        "            'W2':np.random.randn(hidden_2, hidden_1) * np.sqrt(1. / hidden_2),\n",
        "            'W3':np.random.randn(output_layer, hidden_2) * np.sqrt(1. / output_layer)\n",
        "        }\n",
        "\n",
        "        return params\n",
        "\n",
        "    def forward_pass(self, x_train):\n",
        "        params = self.params\n",
        "\n",
        "        # input layer activations becomes sample\n",
        "        params['A0'] = x_train\n",
        "\n",
        "        # input layer to hidden layer 1\n",
        "        try:\n",
        "          params['Z1'] = np.dot(params[\"W1\"], params['A0'])\n",
        "          params['A1'] = self.sigmoid(params['Z1'])\n",
        "        except:\n",
        "          print(\"errorrrr\")\n",
        "\n",
        "        # hidden layer 1 to hidden layer 2\n",
        "        params['Z2'] = np.dot(params[\"W2\"], params['A1'])\n",
        "        params['A2'] = self.sigmoid(params['Z2'])\n",
        "\n",
        "        # hidden layer 2 to output layer\n",
        "        params['Z3'] = np.dot(params[\"W3\"], params['A2'])\n",
        "        params['A3'] = self.softmax(params['Z3'])\n",
        "\n",
        "        return params['A3']\n",
        "\n",
        "    def backward_pass(self, y_train, output):\n",
        "        '''\n",
        "            This is the backpropagation algorithm, for calculating the updates\n",
        "            of the neural network's parameters.\n",
        "\n",
        "            Note: There is a stability issue that causes warnings. This is \n",
        "                  caused  by the dot and multiply operations on the huge arrays.\n",
        "                  \n",
        "                  RuntimeWarning: invalid value encountered in true_divide\n",
        "                  RuntimeWarning: overflow encountered in exp\n",
        "                  RuntimeWarning: overflow encountered in square\n",
        "        '''\n",
        "        params = self.params\n",
        "        change_w = {}\n",
        "\n",
        "        # Calculate W3 update\n",
        "        error = 2 * (output - y_train) / output.shape[0] * self.softmax(params['Z3'], derivative=True)\n",
        "        change_w['W3'] = np.outer(error, params['A2'])\n",
        "\n",
        "        # Calculate W2 update\n",
        "        error = np.dot(params['W3'].T, error) * self.sigmoid(params['Z2'], derivative=True)\n",
        "        change_w['W2'] = np.outer(error, params['A1'])\n",
        "\n",
        "        # Calculate W1 update\n",
        "        error = np.dot(params['W2'].T, error) * self.sigmoid(params['Z1'], derivative=True)\n",
        "        change_w['W1'] = np.outer(error, params['A0'])\n",
        "\n",
        "        return change_w\n",
        "\n",
        "    def update_network_parameters(self, changes_to_w):\n",
        "        '''\n",
        "            Update network parameters according to update rule from\n",
        "            Stochastic Gradient Descent.\n",
        "\n",
        "            θ = θ - η * ∇J(x, y), \n",
        "                theta θ:            a network parameter (e.g. a weight w)\n",
        "                eta η:              the learning rate\n",
        "                gradient ∇J(x, y):  the gradient of the objective function,\n",
        "                                    i.e. the change for a specific theta θ\n",
        "        '''\n",
        "        \n",
        "        for key, value in changes_to_w.items():\n",
        "            self.params[key] -= self.l_rate * value\n",
        "\n",
        "    def compute_accuracy(self, x_val, y_val):\n",
        "        '''\n",
        "            This function does a forward pass of x, then checks if the indices\n",
        "            of the maximum value in the output equals the indices in the label\n",
        "            y. Then it sums over each prediction and calculates the accuracy.\n",
        "        '''\n",
        "        predictions = []\n",
        "\n",
        "        for x, y in zip(x_val, y_val):\n",
        "            output = self.forward_pass(x)\n",
        "            pred = np.argmax(output)\n",
        "            # print(pred , y)\n",
        "            # if not pred == np.argmax(y):\n",
        "              # print(pred , y )\n",
        "            predictions.append(pred == np.argmax(y))\n",
        "            # predictions.append(pred == y)\n",
        "        \n",
        "        return np.mean(predictions)\n",
        "\n",
        "    def train(self, x_train, y_train, x_val, y_val):\n",
        "        start_time = time.time()\n",
        "        for iteration in range(self.epochs):\n",
        "            for x,y in zip(x_train, y_train):\n",
        "                output = self.forward_pass(x)\n",
        "                changes_to_w = self.backward_pass(y, output)\n",
        "                self.update_network_parameters(changes_to_w)\n",
        "            \n",
        "            accuracy = self.compute_accuracy(x_val, y_val)\n",
        "            accuracy2 = self.compute_accuracy(x_train, y_train)\n",
        "            print('Epoch: {0}, Time Spent: {1:.2f}s, Accuracy: {2:.2f}%'.format(\n",
        "                iteration+1, time.time() - start_time, accuracy * 100\n",
        "            ))\n",
        "            print(accuracy2 * 100)\n",
        "            \n",
        "dnn = DeepNeuralNetwork(sizes=[784, 128, 64, 10])\n",
        "dnn.train(x_train, y_train, x_test, y_test)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Time Spent: 59.94s, Accuracy: 80.10%\n",
            "81.035\n",
            "Epoch: 2, Time Spent: 119.78s, Accuracy: 82.53%\n",
            "83.85333333333334\n",
            "Epoch: 3, Time Spent: 179.01s, Accuracy: 83.49%\n",
            "84.89333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-M2wVHGuBro"
      },
      "source": [
        "# part B\n",
        "\n",
        "ref: https://mlfromscratch.com/activation-functions-explained/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ogz9A0vKelu9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee86f7b7-ec87-4b50-c6ce-ac3fe5b62279"
      },
      "source": [
        "# from sklearn.datasets import fetch_openml\n",
        "# from keras.utils.np_utils import to_categorical\n",
        "# import numpy as np\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# import time\n",
        "\n",
        "# x, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
        "# x = (x/255).astype('float32')\n",
        "# y = to_categorical(y)\n",
        "\n",
        "# x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.15, random_state=42)\n",
        "\n",
        "class DeepNeuralNetwork():\n",
        "    def __init__(self, sizes, epochs=3, l_rate=0.1):\n",
        "        self.sizes = sizes\n",
        "        self.epochs = epochs\n",
        "        self.l_rate = l_rate\n",
        "\n",
        "        # we save all parameters in the neural network in this dictionary\n",
        "        self.params = self.initialization()\n",
        "\n",
        "    def sigmoid(self, x, derivative=False):\n",
        "        if derivative:\n",
        "            return (np.exp(-x))/((np.exp(-x)+1)**2)\n",
        "        return 1/(1 + np.exp(-x))\n",
        "    \n",
        "    def relu(self, x, derivative=False):\n",
        "      try:\n",
        "        if derivative:\n",
        "          output = np.ones_like(x)\n",
        "          output[x <= 0] = 0\n",
        "        else:\n",
        "          output = np.maximum(0,x)\n",
        "      except:\n",
        "        print(x.shape)\n",
        "\n",
        "      return output\n",
        "\n",
        "    def softmax(self, x, derivative=False):\n",
        "        # Numerically stable with large exponentials\n",
        "        exps = np.exp(x - x.max())\n",
        "        if derivative:\n",
        "            return exps / np.sum(exps, axis=0) * (1 - exps / np.sum(exps, axis=0))\n",
        "        return exps / np.sum(exps, axis=0)\n",
        "\n",
        "    def initialization(self):\n",
        "        # number of nodes in each layer\n",
        "        input_layer=self.sizes[0]\n",
        "        hidden_1=self.sizes[1]\n",
        "        hidden_2=self.sizes[2]\n",
        "        output_layer=self.sizes[3]\n",
        "\n",
        "        params = {\n",
        "            'W1':np.random.randn(hidden_1, input_layer) * np.sqrt(1. / hidden_1),\n",
        "            'W2':np.random.randn(hidden_2, hidden_1) * np.sqrt(1. / hidden_2),\n",
        "            'W3':np.random.randn(output_layer, hidden_2) * np.sqrt(1. / output_layer)\n",
        "        }\n",
        "\n",
        "        return params\n",
        "\n",
        "    def forward_pass(self, x_train):\n",
        "        params = self.params\n",
        "\n",
        "        # input layer activations becomes sample\n",
        "        params['A0'] = x_train\n",
        "\n",
        "        # input layer to hidden layer 1\n",
        "        try:\n",
        "          params['Z1'] = np.dot(params[\"W1\"], params['A0'])\n",
        "          params['A1'] = self.relu(params['Z1'])\n",
        "        except:\n",
        "          print(\"errorrrr\")\n",
        "\n",
        "        # hidden layer 1 to hidden layer 2\n",
        "        params['Z2'] = np.dot(params[\"W2\"], params['A1'])\n",
        "        params['A2'] = self.relu(params['Z2'])\n",
        "\n",
        "        # hidden layer 2 to output layer\n",
        "        params['Z3'] = np.dot(params[\"W3\"], params['A2'])\n",
        "        params['A3'] = self.relu(params['Z3'])\n",
        "\n",
        "        return params['A3']\n",
        "\n",
        "    def backward_pass(self, y_train, output):\n",
        "        '''\n",
        "            This is the backpropagation algorithm, for calculating the updates\n",
        "            of the neural network's parameters.\n",
        "\n",
        "            Note: There is a stability issue that causes warnings. This is \n",
        "                  caused  by the dot and multiply operations on the huge arrays.\n",
        "                  \n",
        "                  RuntimeWarning: invalid value encountered in true_divide\n",
        "                  RuntimeWarning: overflow encountered in exp\n",
        "                  RuntimeWarning: overflow encountered in square\n",
        "        '''\n",
        "        params = self.params\n",
        "        change_w = {}\n",
        "\n",
        "        # Calculate W3 update\n",
        "        error = 2 * (output - y_train) / output.shape[0] * self.softmax(params['Z3'], derivative=True)\n",
        "        change_w['W3'] = np.outer(error, params['A2'])\n",
        "\n",
        "        # Calculate W2 update\n",
        "        error = np.dot(params['W3'].T, error) * self.relu(params['Z2'], derivative=True)\n",
        "        change_w['W2'] = np.outer(error, params['A1'])\n",
        "\n",
        "        # Calculate W1 update\n",
        "        error = np.dot(params['W2'].T, error) * self.relu(params['Z1'], derivative=True)\n",
        "        change_w['W1'] = np.outer(error, params['A0'])\n",
        "\n",
        "        return change_w\n",
        "\n",
        "    def update_network_parameters(self, changes_to_w):\n",
        "        '''\n",
        "            Update network parameters according to update rule from\n",
        "            Stochastic Gradient Descent.\n",
        "\n",
        "            θ = θ - η * ∇J(x, y), \n",
        "                theta θ:            a network parameter (e.g. a weight w)\n",
        "                eta η:              the learning rate\n",
        "                gradient ∇J(x, y):  the gradient of the objective function,\n",
        "                                    i.e. the change for a specific theta θ\n",
        "        '''\n",
        "        \n",
        "        for key, value in changes_to_w.items():\n",
        "            self.params[key] -= self.l_rate * value\n",
        "\n",
        "    def compute_accuracy(self, x_val, y_val):\n",
        "        '''\n",
        "            This function does a forward pass of x, then checks if the indices\n",
        "            of the maximum value in the output equals the indices in the label\n",
        "            y. Then it sums over each prediction and calculates the accuracy.\n",
        "        '''\n",
        "        predictions = []\n",
        "\n",
        "        for x, y in zip(x_val, y_val):\n",
        "            output = self.forward_pass(x)\n",
        "            pred = np.argmax(output)\n",
        "            # print(pred , y)\n",
        "            # if not pred == np.argmax(y):\n",
        "              # print(pred , y )\n",
        "            predictions.append(pred == np.argmax(y))\n",
        "            # predictions.append(pred == y)\n",
        "        \n",
        "        return np.mean(predictions)\n",
        "\n",
        "    def train(self, x_train, y_train, x_val, y_val):\n",
        "        start_time = time.time()\n",
        "        for iteration in range(self.epochs):\n",
        "            for x,y in zip(x_train, y_train):\n",
        "                output = self.forward_pass(x)\n",
        "                changes_to_w = self.backward_pass(y, output)\n",
        "                self.update_network_parameters(changes_to_w)\n",
        "            \n",
        "            accuracy = self.compute_accuracy(x_val, y_val)\n",
        "            accuracy2 = self.compute_accuracy(x_train, y_train)\n",
        "            print('Epoch: {0}, Time Spent: {1:.2f}s, Accuracy: {2:.2f}%'.format(\n",
        "                iteration+1, time.time() - start_time, accuracy * 100\n",
        "            ))\n",
        "            print(accuracy2 * 100)\n",
        "            \n",
        "dnn = DeepNeuralNetwork(sizes=[784, 128, 64, 10])\n",
        "dnn.train(x_train, y_train, x_test, y_test)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Time Spent: 52.56s, Accuracy: 82.82%\n",
            "84.02\n",
            "Epoch: 2, Time Spent: 105.10s, Accuracy: 84.34%\n",
            "86.08333333333333\n",
            "Epoch: 3, Time Spent: 158.04s, Accuracy: 85.14%\n",
            "87.18333333333334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9LUxpVMxDK3",
        "outputId": "7543cec2-a9c2-4332-cdb9-01095f6ea14e"
      },
      "source": [
        "# from sklearn.datasets import fetch_openml\n",
        "# from keras.utils.np_utils import to_categorical\n",
        "# import numpy as np\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# import time\n",
        "\n",
        "# x, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
        "# x = (x/255).astype('float32')\n",
        "# y = to_categorical(y)\n",
        "\n",
        "# x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.15, random_state=42)\n",
        "\n",
        "class DeepNeuralNetwork():\n",
        "    def __init__(self, sizes, epochs=3, l_rate=0.1):\n",
        "        self.sizes = sizes\n",
        "        self.epochs = epochs\n",
        "        self.l_rate = l_rate\n",
        "\n",
        "        # we save all parameters in the neural network in this dictionary\n",
        "        self.params = self.initialization()\n",
        "\n",
        "    def sigmoid(self, x, derivative=False):\n",
        "        if derivative:\n",
        "            return (np.exp(-x))/((np.exp(-x)+1)**2)\n",
        "        return 1/(1 + np.exp(-x))\n",
        "    \n",
        "    def relu(self, x, derivative=False):\n",
        "      try:\n",
        "        if derivative:\n",
        "          output = np.ones_like(x)\n",
        "          output[x <= 0] = 0\n",
        "        else:\n",
        "          output = np.maximum(0,x)\n",
        "      except:\n",
        "        print(x.shape)\n",
        "\n",
        "      return output\n",
        "\n",
        "    def softmax(self, x, derivative=False):\n",
        "        # Numerically stable with large exponentials\n",
        "        exps = np.exp(x - x.max())\n",
        "        if derivative:\n",
        "            return exps / np.sum(exps, axis=0) * (1 - exps / np.sum(exps, axis=0))\n",
        "        return exps / np.sum(exps, axis=0)\n",
        "\n",
        "    def initialization(self):\n",
        "        # number of nodes in each layer\n",
        "        input_layer=self.sizes[0]\n",
        "        hidden_1=self.sizes[1]\n",
        "        hidden_2=self.sizes[2]\n",
        "        hidden_3=self.sizes[3]\n",
        "        output_layer=self.sizes[4]\n",
        "\n",
        "        params = {\n",
        "            'W1':np.random.randn(hidden_1, input_layer) * np.sqrt(1. / hidden_1),\n",
        "            'W2':np.random.randn(hidden_2, hidden_1) * np.sqrt(1. / hidden_2),\n",
        "            'W3':np.random.randn(hidden_3, hidden_2) * np.sqrt(1. / hidden_3),\n",
        "            'W4':np.random.randn(output_layer, hidden_3) * np.sqrt(1. / output_layer)\n",
        "        }\n",
        "\n",
        "        return params\n",
        "\n",
        "    def forward_pass(self, x_train):\n",
        "        params = self.params\n",
        "\n",
        "        # input layer activations becomes sample\n",
        "        params['A0'] = x_train\n",
        "\n",
        "        # input layer to hidden layer 1\n",
        "        params['Z1'] = np.dot(params[\"W1\"], params['A0'])\n",
        "        params['A1'] = self.relu(params['Z1'])\n",
        "\n",
        "        # hidden layer 1 to hidden layer 2\n",
        "        params['Z2'] = np.dot(params[\"W2\"], params['A1'])\n",
        "        params['A2'] = self.relu(params['Z2'])\n",
        "\n",
        "        # hidden layer 2 to hidden layer 3\n",
        "        params['Z3'] = np.dot(params[\"W3\"], params['A2'])\n",
        "        params['A3'] = self.relu(params['Z3'])\n",
        "\n",
        "        # hidden layer 2 to output layer\n",
        "        params['Z4'] = np.dot(params[\"W4\"], params['A3'])\n",
        "        params['A4'] = self.relu(params['Z4'])\n",
        "\n",
        "        return params['A4']\n",
        "\n",
        "    def backward_pass(self, y_train, output):\n",
        "        '''\n",
        "            This is the backpropagation algorithm, for calculating the updates\n",
        "            of the neural network's parameters.\n",
        "\n",
        "            Note: There is a stability issue that causes warnings. This is \n",
        "                  caused  by the dot and multiply operations on the huge arrays.\n",
        "                  \n",
        "                  RuntimeWarning: invalid value encountered in true_divide\n",
        "                  RuntimeWarning: overflow encountered in exp\n",
        "                  RuntimeWarning: overflow encountered in square\n",
        "        '''\n",
        "        params = self.params\n",
        "        change_w = {}\n",
        "\n",
        "        # Calculate W4 update\n",
        "        error = 2 * (output - y_train) / output.shape[0] * self.softmax(params['Z4'], derivative=True)\n",
        "        change_w['W4'] = np.outer(error, params['A3'])\n",
        "\n",
        "        # Calculate W3 update\n",
        "        error = np.dot(params['W4'].T, error) * self.relu(params['Z3'], derivative=True)\n",
        "        change_w['W3'] = np.outer(error, params['A2'])\n",
        "\n",
        "        # Calculate W2 update\n",
        "        error = np.dot(params['W3'].T, error) * self.relu(params['Z2'], derivative=True)\n",
        "        change_w['W2'] = np.outer(error, params['A1'])\n",
        "\n",
        "        # Calculate W1 update\n",
        "        error = np.dot(params['W2'].T, error) * self.relu(params['Z1'], derivative=True)\n",
        "        change_w['W1'] = np.outer(error, params['A0'])\n",
        "\n",
        "        return change_w\n",
        "\n",
        "    def update_network_parameters(self, changes_to_w):\n",
        "        '''\n",
        "            Update network parameters according to update rule from\n",
        "            Stochastic Gradient Descent.\n",
        "\n",
        "            θ = θ - η * ∇J(x, y), \n",
        "                theta θ:            a network parameter (e.g. a weight w)\n",
        "                eta η:              the learning rate\n",
        "                gradient ∇J(x, y):  the gradient of the objective function,\n",
        "                                    i.e. the change for a specific theta θ\n",
        "        '''\n",
        "        \n",
        "        for key, value in changes_to_w.items():\n",
        "            self.params[key] -= self.l_rate * value\n",
        "\n",
        "    def compute_accuracy(self, x_val, y_val):\n",
        "        '''\n",
        "            This function does a forward pass of x, then checks if the indices\n",
        "            of the maximum value in the output equals the indices in the label\n",
        "            y. Then it sums over each prediction and calculates the accuracy.\n",
        "        '''\n",
        "        predictions = []\n",
        "\n",
        "        for x, y in zip(x_val, y_val):\n",
        "            output = self.forward_pass(x)\n",
        "            pred = np.argmax(output)\n",
        "            # print(pred , y)\n",
        "            # if not pred == np.argmax(y):\n",
        "              # print(pred , y )\n",
        "            predictions.append(pred == np.argmax(y))\n",
        "            # predictions.append(pred == y)\n",
        "        \n",
        "        return np.mean(predictions)\n",
        "\n",
        "    def train(self, x_train, y_train, x_val, y_val):\n",
        "        start_time = time.time()\n",
        "        for iteration in range(self.epochs):\n",
        "            for x,y in zip(x_train, y_train):\n",
        "                output = self.forward_pass(x)\n",
        "                changes_to_w = self.backward_pass(y, output)\n",
        "                self.update_network_parameters(changes_to_w)\n",
        "            \n",
        "            accuracy = self.compute_accuracy(x_val, y_val)\n",
        "            accuracy2 = self.compute_accuracy(x_train, y_train)\n",
        "            print('Epoch: {0}, Time Spent: {1:.2f}s, Accuracy: {2:.2f}%'.format(\n",
        "                iteration+1, time.time() - start_time, accuracy * 100\n",
        "            ))\n",
        "            print(accuracy2 * 100)\n",
        "            \n",
        "dnn = DeepNeuralNetwork(sizes=[784, 128, 64, 64, 10])\n",
        "dnn.train(x_train, y_train, x_test, y_test)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Time Spent: 58.98s, Accuracy: 82.50%\n",
            "83.94833333333334\n",
            "Epoch: 2, Time Spent: 117.58s, Accuracy: 84.29%\n",
            "86.02833333333334\n",
            "Epoch: 3, Time Spent: 176.21s, Accuracy: 85.05%\n",
            "87.10166666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8g00wVd0S13i"
      },
      "source": [
        "# Pytorch\n",
        "ref: https://github.com/iam-mhaseeb/Multi-Layer-Perceptron-MNIST-with-PyTorch/blob/master/mnist_mlp_exercise.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFRIdexetFwE"
      },
      "source": [
        "import torch "
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-B5nJUqtv-P5"
      },
      "source": [
        "import torchvision\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# number of subprocesses to use for data loading\n",
        "num_workers = 0\n",
        "# how many samples per batch to load\n",
        "batch_size = 20\n",
        "\n",
        "# convert data to torch.FloatTensor\n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "# choose the training and test datasets\n",
        "train_data = datasets.FashionMNIST(root='data', train=True,\n",
        "                                   download=True, transform=transform)\n",
        "test_data = datasets.FashionMNIST(root='data', train=False,\n",
        "                                  download=True, transform=transform)\n",
        "\n",
        "# prepare data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
        "    num_workers=num_workers)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n",
        "    num_workers=num_workers)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFI9rRhvwDxM",
        "outputId": "2c641b1d-c26d-4bc8-d361-533316d9e6e1"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "## Define the NN architecture\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 512)\n",
        "        # linear layer (n_hidden -> hidden_2)\n",
        "        self.fc2 = nn.Linear(512, 512)\n",
        "        # linear layer (n_hidden -> 10)\n",
        "        self.fc3 = nn.Linear(512, 10)\n",
        "        # dropout layer (p=0.2)\n",
        "        # dropout prevents overfitting of data\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # flatten image input\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        # add hidden layer, with relu activation function\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return x\n",
        "\n",
        "# initialize the NN\n",
        "model = Net()\n",
        "print(model)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
            "  (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
            "  (fc3): Linear(in_features=512, out_features=10, bias=True)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLqGdWXQwFv_"
      },
      "source": [
        "## Specify loss and optimization functions\n",
        "\n",
        "# specify loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# specify optimizer\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2B9ptfrwSmF",
        "outputId": "44b1a81d-1c9b-4dc2-c78f-07e3ff3d6633"
      },
      "source": [
        "# number of epochs to train the model\n",
        "n_epochs = 3  # suggest training between 20-50 epochs\n",
        "\n",
        "model.train() # prep model for training\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    # monitor training loss\n",
        "    train_loss = 0.0\n",
        "    \n",
        "    ###################\n",
        "    # train the model #\n",
        "    ###################\n",
        "    for data, target in train_loader:\n",
        "        # clear the gradients of all optimized variables\n",
        "        optimizer.zero_grad()\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model(data)\n",
        "        # calculate the loss\n",
        "        loss = criterion(output, target)\n",
        "        # backward pass: compute gradient of the loss with respect to model parameters\n",
        "        loss.backward()\n",
        "        # perform a single optimization step (parameter update)\n",
        "        optimizer.step()\n",
        "        # update running training loss\n",
        "        train_loss += loss.item()*data.size(0)\n",
        "        \n",
        "    # print training statistics \n",
        "    # calculate average loss over an epoch\n",
        "    train_loss = train_loss/len(train_loader.dataset)\n",
        "\n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
        "        epoch+1, \n",
        "        train_loss\n",
        "        ))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 \tTraining Loss: 0.532361\n",
            "Epoch: 2 \tTraining Loss: 0.500572\n",
            "Epoch: 3 \tTraining Loss: 0.481839\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbyJ3V91SYe4",
        "outputId": "683aec7b-7329-4baa-f642-8bf6694c1f56"
      },
      "source": [
        "# initialize lists to monitor test loss and accuracy\n",
        "test_loss = 0.0\n",
        "class_correct = list(0. for i in range(10))\n",
        "class_total = list(0. for i in range(10))\n",
        "\n",
        "model.eval() # prep model for *evaluation*\n",
        "\n",
        "for data, target in test_loader:\n",
        "    # forward pass: compute predicted outputs by passing inputs to the model\n",
        "    output = model(data)\n",
        "    # calculate the loss\n",
        "    loss = criterion(output, target)\n",
        "    # update test loss \n",
        "    test_loss += loss.item()*data.size(0)\n",
        "    # convert output probabilities to predicted class\n",
        "    _, pred = torch.max(output, 1)\n",
        "    # compare predictions to true label\n",
        "    correct = np.squeeze(pred.eq(target.data.view_as(pred)))\n",
        "    # calculate test accuracy for each object class\n",
        "    for i in range(batch_size):\n",
        "        label = target.data[i]\n",
        "        class_correct[label] += correct[i].item()\n",
        "        class_total[label] += 1\n",
        "\n",
        "# calculate and print avg test loss\n",
        "test_loss = test_loss/len(test_loader.dataset)\n",
        "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
        "\n",
        "for i in range(10):\n",
        "    if class_total[i] > 0:\n",
        "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
        "            str(i), 100 * class_correct[i] / class_total[i],\n",
        "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
        "    else:\n",
        "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
        "\n",
        "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
        "    100. * np.sum(class_correct) / np.sum(class_total),\n",
        "    np.sum(class_correct), np.sum(class_total)))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.501188\n",
            "\n",
            "Test Accuracy of     0: 81% (818/1000)\n",
            "Test Accuracy of     1: 94% (944/1000)\n",
            "Test Accuracy of     2: 72% (724/1000)\n",
            "Test Accuracy of     3: 87% (871/1000)\n",
            "Test Accuracy of     4: 77% (771/1000)\n",
            "Test Accuracy of     5: 88% (880/1000)\n",
            "Test Accuracy of     6: 50% (507/1000)\n",
            "Test Accuracy of     7: 90% (907/1000)\n",
            "Test Accuracy of     8: 94% (943/1000)\n",
            "Test Accuracy of     9: 95% (950/1000)\n",
            "\n",
            "Test Accuracy (Overall): 83% (8315/10000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syj-ZqZNScqz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzCzaUnNTQMw"
      },
      "source": [
        "# momentum"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dM7QgDSOTSgm",
        "outputId": "50271651-4885-4737-ac50-4804ace39242"
      },
      "source": [
        "# from sklearn.datasets import fetch_openml\n",
        "# from keras.utils.np_utils import to_categorical\n",
        "# import numpy as np\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# import time\n",
        "\n",
        "# x, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
        "# x = (x/255).astype('float32')\n",
        "# y = to_categorical(y)\n",
        "\n",
        "# x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.15, random_state=42)\n",
        "\n",
        "class DeepNeuralNetwork():\n",
        "    def __init__(self, sizes, epochs=3, l_rate=0.1,m_rate=0.1):\n",
        "        self.sizes = sizes\n",
        "        self.epochs = epochs\n",
        "        self.l_rate = l_rate\n",
        "        self.m_rate = m_rate\n",
        "        self.m_val = {}\n",
        "\n",
        "        # we save all parameters in the neural network in this dictionary\n",
        "        self.params = self.initialization()\n",
        "\n",
        "    def sigmoid(self, x, derivative=False):\n",
        "        if derivative:\n",
        "            return (np.exp(-x))/((np.exp(-x)+1)**2)\n",
        "        return 1/(1 + np.exp(-x))\n",
        "\n",
        "    def softmax(self, x, derivative=False):\n",
        "        # Numerically stable with large exponentials\n",
        "        exps = np.exp(x - x.max())\n",
        "        if derivative:\n",
        "            return exps / np.sum(exps, axis=0) * (1 - exps / np.sum(exps, axis=0))\n",
        "        return exps / np.sum(exps, axis=0)\n",
        "\n",
        "    def initialization(self):\n",
        "        # number of nodes in each layer\n",
        "        input_layer=self.sizes[0]\n",
        "        hidden_1=self.sizes[1]\n",
        "        hidden_2=self.sizes[2]\n",
        "        output_layer=self.sizes[3]\n",
        "\n",
        "        params = {\n",
        "            'W1':np.random.randn(hidden_1, input_layer) * np.sqrt(1. / hidden_1),\n",
        "            'W2':np.random.randn(hidden_2, hidden_1) * np.sqrt(1. / hidden_2),\n",
        "            'W3':np.random.randn(output_layer, hidden_2) * np.sqrt(1. / output_layer)\n",
        "        }\n",
        "\n",
        "        return params\n",
        "\n",
        "    def forward_pass(self, x_train):\n",
        "        params = self.params\n",
        "\n",
        "        # input layer activations becomes sample\n",
        "        params['A0'] = x_train\n",
        "\n",
        "        # input layer to hidden layer 1\n",
        "        try:\n",
        "          params['Z1'] = np.dot(params[\"W1\"], params['A0'])\n",
        "          params['A1'] = self.sigmoid(params['Z1'])\n",
        "        except:\n",
        "          print(\"errorrrr\")\n",
        "\n",
        "        # hidden layer 1 to hidden layer 2\n",
        "        params['Z2'] = np.dot(params[\"W2\"], params['A1'])\n",
        "        params['A2'] = self.sigmoid(params['Z2'])\n",
        "\n",
        "        # hidden layer 2 to output layer\n",
        "        params['Z3'] = np.dot(params[\"W3\"], params['A2'])\n",
        "        params['A3'] = self.softmax(params['Z3'])\n",
        "\n",
        "        return params['A3']\n",
        "\n",
        "    def backward_pass(self, y_train, output):\n",
        "        '''\n",
        "            This is the backpropagation algorithm, for calculating the updates\n",
        "            of the neural network's parameters.\n",
        "\n",
        "            Note: There is a stability issue that causes warnings. This is \n",
        "                  caused  by the dot and multiply operations on the huge arrays.\n",
        "                  \n",
        "                  RuntimeWarning: invalid value encountered in true_divide\n",
        "                  RuntimeWarning: overflow encountered in exp\n",
        "                  RuntimeWarning: overflow encountered in square\n",
        "        '''\n",
        "        params = self.params\n",
        "        change_w = {}\n",
        "\n",
        "        # Calculate W3 update\n",
        "        error = 2 * (output - y_train) / output.shape[0] * self.softmax(params['Z3'], derivative=True)\n",
        "        change_w['W3'] = np.outer(error, params['A2'])\n",
        "\n",
        "        # Calculate W2 update\n",
        "        error = np.dot(params['W3'].T, error) * self.sigmoid(params['Z2'], derivative=True)\n",
        "        change_w['W2'] = np.outer(error, params['A1'])\n",
        "\n",
        "        # Calculate W1 update\n",
        "        error = np.dot(params['W2'].T, error) * self.sigmoid(params['Z1'], derivative=True)\n",
        "        change_w['W1'] = np.outer(error, params['A0'])\n",
        "\n",
        "        return change_w\n",
        "\n",
        "    def update_network_parameters(self, changes_to_w):\n",
        "        '''\n",
        "            Update network parameters according to update rule from\n",
        "            Stochastic Gradient Descent.\n",
        "\n",
        "            θ = θ - η * ∇J(x, y), \n",
        "                theta θ:            a network parameter (e.g. a weight w)\n",
        "                eta η:              the learning rate\n",
        "                gradient ∇J(x, y):  the gradient of the objective function,\n",
        "                                    i.e. the change for a specific theta θ\n",
        "        '''\n",
        "        \n",
        "        for key, value in changes_to_w.items():\n",
        "            self.params[key] -= self.l_rate * value\n",
        "\n",
        "    def update_network_parameters_momentum(self, changes_to_w):\n",
        "\n",
        "        \n",
        "        for key, value in changes_to_w.items():\n",
        "            try:\n",
        "               self.m_val[key] = self.l_rate * value + self.m_rate * self.m_val[key]\n",
        "            except:\n",
        "              self.m_val[key] = self.l_rate * value \n",
        "\n",
        "            self.params[key] -= (self.m_val[key])\n",
        "\n",
        "    def compute_accuracy(self, x_val, y_val):\n",
        "        '''\n",
        "            This function does a forward pass of x, then checks if the indices\n",
        "            of the maximum value in the output equals the indices in the label\n",
        "            y. Then it sums over each prediction and calculates the accuracy.\n",
        "        '''\n",
        "        predictions = []\n",
        "\n",
        "        for x, y in zip(x_val, y_val):\n",
        "            output = self.forward_pass(x)\n",
        "            pred = np.argmax(output)\n",
        "            # print(pred , y)\n",
        "            # if not pred == np.argmax(y):\n",
        "              # print(pred , y )\n",
        "            predictions.append(pred == np.argmax(y))\n",
        "            # predictions.append(pred == y)\n",
        "        \n",
        "        return np.mean(predictions)\n",
        "\n",
        "    def train(self, x_train, y_train, x_val, y_val):\n",
        "        start_time = time.time()\n",
        "        for iteration in range(self.epochs):\n",
        "            for x,y in zip(x_train, y_train):\n",
        "                output = self.forward_pass(x)\n",
        "                changes_to_w = self.backward_pass(y, output) \n",
        "                self.update_network_parameters_momentum(changes_to_w)\n",
        "            \n",
        "            accuracy = self.compute_accuracy(x_val, y_val)\n",
        "            accuracy2 = self.compute_accuracy(x_train, y_train)\n",
        "            print('Epoch: {0}, Time Spent: {1:.2f}s, Accuracy: {2:.2f}%'.format(\n",
        "                iteration+1, time.time() - start_time, accuracy * 100\n",
        "            ))\n",
        "            print(accuracy2 * 100)\n",
        "            \n",
        "dnn = DeepNeuralNetwork(sizes=[784, 128, 64, 10])\n",
        "dnn.train(x_train, y_train, x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Time Spent: 79.90s, Accuracy: 80.30%\n",
            "81.34666666666666\n",
            "Epoch: 2, Time Spent: 159.62s, Accuracy: 82.60%\n",
            "83.99666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPeNV6sZTh4u"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}